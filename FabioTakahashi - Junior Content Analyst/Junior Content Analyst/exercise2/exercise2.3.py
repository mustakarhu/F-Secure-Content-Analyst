#!/usr/bin/python3

import timeit  # used to measure the runtime of the different approaches
import random  # used to randomize data
import pandas as pd  # used to store DataFrame
from faker import Faker  # used to generate fake data for exemplification
import json  # json module to load the json files
import exercise2  # to call the make_statistics() function

SHOW_OUTPUT = True
SAVE_DIR = 'faker/'
STATISTICS_DIR = 'statistics/'
REMOVE_DUPLICATES = False
n_files = 20


def read_multi():
    """
    Open all the files using json.load function instead of calling pandas pd.read_json
    Gather all data to a single DataFrame and save the statistics to only one json file.
    :return: None
    """

    df = pd.DataFrame()
    for n in range(n_files):
        with open(f'{SAVE_DIR}data{n+1}.json') as f:
            json_dict = json.load(f)
            part_df = pd.DataFrame(data=json_dict).T.reset_index()
            part_df = part_df.rename(columns={'index': 'fullname'})
            df = df.append(part_df)
    # to reduce the runtime for the make_statistics function we can drop the duplicates
    # if they may occur across different files
    if REMOVE_DUPLICATES:
        df.drop_duplicates(inplace=True, keep=False)

    exercise2.make_statistics(df, f'{SAVE_DIR}{STATISTICS_DIR}multi_file_output')


def read_single():
    """
    Read each file to a DataFrame using pd.read_json() process each file individually
    :return: None
    """
    df = pd.DataFrame()
    for n in range(n_files):
        part_df = pd.read_json(f'{SAVE_DIR}data{n+1}.json', orient='records')
        part_df = part_df.T.reset_index()
        part_df = part_df.rename(columns={'index': 'fullname'})
        df = df.append(part_df)
        exercise2.make_statistics(df, f'{SAVE_DIR}{STATISTICS_DIR}single_keep_duplicates_output{n}')


if __name__ == '__main__':
    print('\u001b[36mQ: Imagine you are now given 20 JSON inputs.\
How will you minimize the runtime for the above tasks?\n')

    print('A: Considering that the data is not too large to be stored in the memory, The best approach would be to \n'
          'collect all the data from the different files and store it on a single DataFrame. before doing the \n'
          'statistics. The benefits of this approach are: \n\n'
          '\t1. Possibility of removing duplicated entries that may exist in different files.\n'
          '\t2. We only need to open the file to store the statistics once. This step will dramatically decrease the '
          'runtime for the program as we do not need to perform several open requests to the same file.\n\n '
          'It is important to note however that for extremely large files we may not be able to load multiple json \n'
          'files to a single DataFrame. In this case the second best course of action is to open each file \n'
          'individually and appending the results to the existing statistics file.')

    print('\u001b[0m')

    n_names = 20  # number of names to be generated by Faker
    n_jobs = 20  # number of occupations to be generated by Faker
    n_cities = 20  # number of different cities to be generated by Faker
    n_inputs = 100  # maximum number of entries in each file
    fake = Faker('en_US')
    fake_fi = Faker('fi_FI')
    # we create a collection of data using faker
    cities = set([fake_fi.city().replace('ä', 'a').replace('ö', 'o') for _ in range(n_cities)])
    first_names = set([fake.first_name() for _ in range(n_names)])
    last_names = set([fake.last_name() for _ in range(n_names)])
    occupations = set([fake.job().replace(',', ' ') for _ in range(n_jobs)])
    for file_nr in range(n_files):
        d = {}
        for _ in range(n_inputs):
            key = random.sample(first_names, 1)[0]+' '+random.sample(last_names, 1)[0]
            if key not in d.keys():
                d[key] = {
                    'age': random.randrange(19, 64),
                    'address': random.sample(cities, 1)[0],
                    'occupation': random.sample(occupations, 1)[0],
                }
        pd.DataFrame(data=d).to_json(f'{SAVE_DIR}data{file_nr+1}.json', indent=4)

    n_runs = 5
    # using new approach loading multiple json to a single DataFrame
    print(
        f'Average runtime for combining multiple files to one DataFrame: '
        f'{timeit.timeit("read_multi()", setup="from __main__ import read_multi", number=n_runs)/n_runs} sec.'
    )

    # using approach as in exercise 2.1 and 2.2
    print(
        f'Average runtime for executing statistics on each file individually: '
        f'{timeit.timeit("read_single()", setup="from __main__ import read_single", number=n_runs)/n_runs} sec.'
    )
